{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Getting Familiar with the Dataset\n",
    "### Read in semi-structured text data from a dataset obtained from UCI Machine Learning Repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"ham\\tI've been searching for the right words to thank you for this breather. I promise i wont take your help for granted and will fulfil my promise. You have been wonderful and a blessing at all times.\\nspam\\tFree entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's\\nham\\tNah I don't think he goes to usf, he lives around here though\\nham\\tEven my brother is not like to speak with me. They treat me like aid\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read in the raw text\n",
    "rawData = open(\"SMSSpamCollection.tsv\").read()\n",
    "\n",
    "# Print the raw data\n",
    "rawData[0:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>body_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>I've been searching for the right words to tha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>Even my brother is not like to speak with me. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>I HAVE A DATE ON SUNDAY WITH WILL!!</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                          body_text\n",
       "0   ham  I've been searching for the right words to tha...\n",
       "1  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "2   ham  Nah I don't think he goes to usf, he lives aro...\n",
       "3   ham  Even my brother is not like to speak with me. ...\n",
       "4   ham                I HAVE A DATE ON SUNDAY WITH WILL!!"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv('SMSSpamCollection.tsv', sep='\\t', header=None)\n",
    "data.columns = ['label', 'body_text']\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data has 5568 rows and 2 columns\n",
      "Out of 5568 rows, 746 are spam, 4822 are ham\n",
      "Number of null in label: 0\n",
      "Number of null in text: 0\n"
     ]
    }
   ],
   "source": [
    "# What is the shape of the dataset?\n",
    "\n",
    "print(\"Input data has {} rows and {} columns\".format(len(data), len(data.columns)))\n",
    "\n",
    "# How many spam/ham are there?\n",
    "\n",
    "print(\"Out of {} rows, {} are spam, {} are ham\".format(len(data),\n",
    "                                                       len(data[data['label']=='spam']),\n",
    "                                                       len(data[data['label']=='ham'])))\n",
    "# How much missing data is there?\n",
    "\n",
    "print(\"Number of null in label: {}\".format(data['label'].isnull().sum()))\n",
    "print(\"Number of null in text: {}\".format(data['body_text'].isnull().sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Feature Engineering: adding Features to the data\n",
    "_later on we will clean the text and remove punctuation from it. However, these pieces of information may be helpful for our spam filter somehow. Therefore, it would be efficient to analyze these feature before cleaning the data_ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>body_text</th>\n",
       "      <th>body_len</th>\n",
       "      <th>punct%</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>I've been searching for the right words to tha...</td>\n",
       "      <td>160</td>\n",
       "      <td>2.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>128</td>\n",
       "      <td>4.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>49</td>\n",
       "      <td>4.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>Even my brother is not like to speak with me. ...</td>\n",
       "      <td>62</td>\n",
       "      <td>3.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>I HAVE A DATE ON SUNDAY WITH WILL!!</td>\n",
       "      <td>28</td>\n",
       "      <td>7.1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                          body_text  body_len  punct%\n",
       "0   ham  I've been searching for the right words to tha...       160     2.5\n",
       "1  spam  Free entry in 2 a wkly comp to win FA Cup fina...       128     4.7\n",
       "2   ham  Nah I don't think he goes to usf, he lives aro...        49     4.1\n",
       "3   ham  Even my brother is not like to speak with me. ...        62     3.2\n",
       "4   ham                I HAVE A DATE ON SUNDAY WITH WILL!!        28     7.1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "def count_punct(text):\n",
    "    count = sum([1 for char in text if char in string.punctuation])\n",
    "    return round(count/(len(text) - text.count(\" \")), 3)*100\n",
    "\n",
    "data['body_len'] = data['body_text'].apply(lambda x: len(x) - x.count(\" \"))\n",
    "data['punct%'] = data['body_text'].apply(lambda x: count_punct(x))\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot\n",
    "import numpy as np\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "bins = np.linspace(0, 200, 40)\n",
    "\n",
    "pyplot.hist(data[data['label']=='spam']['body_len'], bins, alpha=0.5,density=True,stacked=True,  label='spam')\n",
    "pyplot.hist(data[data['label']=='ham']['body_len'], bins, alpha=0.5,density=True,stacked=True, label='ham')\n",
    "pyplot.legend(loc='upper right')\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "bins = np.linspace(0, 50, 40)\n",
    "\n",
    "pyplot.hist(data[data['label']=='spam']['punct%'], bins, alpha=0.5, density=True,stacked=True, label='spam')\n",
    "pyplot.hist(data[data['label']=='ham']['punct%'], bins, alpha=0.5, density=True,stacked=True, label='ham')\n",
    "pyplot.legend(loc='upper right')\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: Text Analysis and Cleaning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>body_text</th>\n",
       "      <th>body_len</th>\n",
       "      <th>punct%</th>\n",
       "      <th>body_text_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>I've been searching for the right words to tha...</td>\n",
       "      <td>160</td>\n",
       "      <td>2.5</td>\n",
       "      <td>[ive, searching, right, words, thank, breather...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>128</td>\n",
       "      <td>4.7</td>\n",
       "      <td>[free, entry, 2, wkly, comp, win, fa, cup, fin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>49</td>\n",
       "      <td>4.1</td>\n",
       "      <td>[nah, dont, think, goes, usf, lives, around, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>Even my brother is not like to speak with me. ...</td>\n",
       "      <td>62</td>\n",
       "      <td>3.2</td>\n",
       "      <td>[even, brother, like, speak, treat, like, aids...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>I HAVE A DATE ON SUNDAY WITH WILL!!</td>\n",
       "      <td>28</td>\n",
       "      <td>7.1</td>\n",
       "      <td>[date, sunday]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                          body_text  body_len  punct%  \\\n",
       "0   ham  I've been searching for the right words to tha...       160     2.5   \n",
       "1  spam  Free entry in 2 a wkly comp to win FA Cup fina...       128     4.7   \n",
       "2   ham  Nah I don't think he goes to usf, he lives aro...        49     4.1   \n",
       "3   ham  Even my brother is not like to speak with me. ...        62     3.2   \n",
       "4   ham                I HAVE A DATE ON SUNDAY WITH WILL!!        28     7.1   \n",
       "\n",
       "                                     body_text_clean  \n",
       "0  [ive, searching, right, words, thank, breather...  \n",
       "1  [free, entry, 2, wkly, comp, win, fa, cup, fin...  \n",
       "2  [nah, dont, think, goes, usf, lives, around, t...  \n",
       "3  [even, brother, like, speak, treat, like, aids...  \n",
       "4                                     [date, sunday]  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import re\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "def clean_text(text):\n",
    "    text = \"\".join([word for word in text if word not in string.punctuation])\n",
    "    tokens = re.split('\\W+', text)\n",
    "    text = [word for word in tokens if word not in stopwords]\n",
    "    return text\n",
    "\n",
    "data['body_text_clean'] = data['body_text'].apply(lambda x: clean_text(x.lower()))\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body_text_lemmatized</th>\n",
       "      <th>body_text_stemmed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[ive, searching, right, word, thank, breather,...</td>\n",
       "      <td>[ive, search, right, word, thank, breather, pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[free, entry, 2, wkly, comp, win, fa, cup, fin...</td>\n",
       "      <td>[free, entri, 2, wkli, comp, win, fa, cup, fin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[nah, dont, think, go, usf, life, around, though]</td>\n",
       "      <td>[nah, dont, think, goe, usf, live, around, tho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[even, brother, like, speak, treat, like, aid,...</td>\n",
       "      <td>[even, brother, like, speak, treat, like, aid,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[date, sunday]</td>\n",
       "      <td>[date, sunday]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                body_text_lemmatized  \\\n",
       "0  [ive, searching, right, word, thank, breather,...   \n",
       "1  [free, entry, 2, wkly, comp, win, fa, cup, fin...   \n",
       "2  [nah, dont, think, go, usf, life, around, though]   \n",
       "3  [even, brother, like, speak, treat, like, aid,...   \n",
       "4                                     [date, sunday]   \n",
       "\n",
       "                                   body_text_stemmed  \n",
       "0  [ive, search, right, word, thank, breather, pr...  \n",
       "1  [free, entri, 2, wkli, comp, win, fa, cup, fin...  \n",
       "2  [nah, dont, think, goe, usf, live, around, tho...  \n",
       "3  [even, brother, like, speak, treat, like, aid,...  \n",
       "4                                     [date, sunday]  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ps = nltk.PorterStemmer()\n",
    "wn = nltk.WordNetLemmatizer()\n",
    "\n",
    "def lemmatizing(tokenized_text):\n",
    "    text = [wn.lemmatize(word) for word in tokenized_text]\n",
    "    return text\n",
    "\n",
    "def stemming(tokenized_text):\n",
    "    text = [ps.stem(word) for word in tokenized_text]\n",
    "    return text\n",
    "\n",
    "trial_data = pd.DataFrame()\n",
    "trial_data['body_text_lemmatized'] = data['body_text_clean'].apply(lambda x: lemmatizing(x))\n",
    "trial_data['body_text_stemmed'] = data['body_text_clean'].apply(lambda x: stemming(x))\n",
    "\n",
    "trial_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_from now on, we will move on with the limitizer instead of the stemmer since it makes smarter and more meaningful transformation to the vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4: Data Preprocessing\n",
    "### Method 1: Apply CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5568, 11039)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<5568x11039 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 56278 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def analyze_text(text):\n",
    "    return lemmatizing(clean_text(text))\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "count_vect = CountVectorizer(analyzer=analyze_text)\n",
    "X_counts = count_vect.fit_transform(data['body_text'])\n",
    "print(X_counts.shape)\n",
    "X_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorizers output sparse matrices\n",
    "\n",
    "_**Sparse Matrix**: A matrix in which most entries are 0. In the interest of efficient storage, a sparse matrix will be stored by only storing the locations of the non-zero elements. We will use a dataframe instead to better represent the sparse matrix_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body_len</th>\n",
       "      <th>punct%</th>\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>008704050406</th>\n",
       "      <th>0089my</th>\n",
       "      <th>0121</th>\n",
       "      <th>01223585236</th>\n",
       "      <th>01223585334</th>\n",
       "      <th>0125698789</th>\n",
       "      <th>...</th>\n",
       "      <th>zero</th>\n",
       "      <th>zhong</th>\n",
       "      <th>zoom</th>\n",
       "      <th>zyada</th>\n",
       "      <th>Ü</th>\n",
       "      <th>Üll</th>\n",
       "      <th>é</th>\n",
       "      <th>ü</th>\n",
       "      <th>üll</th>\n",
       "      <th>〨ud</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>160</td>\n",
       "      <td>2.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>128</td>\n",
       "      <td>4.7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>49</td>\n",
       "      <td>4.1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>62</td>\n",
       "      <td>3.2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28</td>\n",
       "      <td>7.1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 11041 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   body_len  punct%     0  008704050406  0089my  0121  01223585236  \\\n",
       "0       160     2.5  0  0             0       0     0            0   \n",
       "1       128     4.7  0  0             0       0     0            0   \n",
       "2        49     4.1  0  0             0       0     0            0   \n",
       "3        62     3.2  0  0             0       0     0            0   \n",
       "4        28     7.1  0  0             0       0     0            0   \n",
       "\n",
       "   01223585334  0125698789  ...  zero  zhong  zoom  zyada  Ü  Üll  é  ü  üll  \\\n",
       "0            0           0  ...     0      0     0      0  0    0  0  0    0   \n",
       "1            0           0  ...     0      0     0      0  0    0  0  0    0   \n",
       "2            0           0  ...     0      0     0      0  0    0  0  0    0   \n",
       "3            0           0  ...     0      0     0      0  0    0  0  0    0   \n",
       "4            0           0  ...     0      0     0      0  0    0  0  0    0   \n",
       "\n",
       "   〨ud  \n",
       "0    0  \n",
       "1    0  \n",
       "2    0  \n",
       "3    0  \n",
       "4    0  \n",
       "\n",
       "[5 rows x 11041 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_counts_feat = pd.DataFrame(X_counts.toarray())\n",
    "X_counts_feat.columns = count_vect.get_feature_names()\n",
    "X_counts_feat=pd.concat([data['body_len'], data['punct%'], X_counts_feat],axis =1)\n",
    "X_counts_feat.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 2: Apply TF-IDF\n",
    "\n",
    "Creates a document-term matrix where the columns represent single unique terms (unigrams) but the cell represents a weighting meant to represent how important a word is to a document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5568, 11039)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<5568x11039 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 56278 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vect = TfidfVectorizer(analyzer=analyze_text)\n",
    "X_tfidf = tfidf_vect.fit_transform(data['body_text'])\n",
    "print(X_tfidf.shape)\n",
    "X_tfidf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body_len</th>\n",
       "      <th>punct%</th>\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>008704050406</th>\n",
       "      <th>0089my</th>\n",
       "      <th>0121</th>\n",
       "      <th>01223585236</th>\n",
       "      <th>01223585334</th>\n",
       "      <th>0125698789</th>\n",
       "      <th>...</th>\n",
       "      <th>zero</th>\n",
       "      <th>zhong</th>\n",
       "      <th>zoom</th>\n",
       "      <th>zyada</th>\n",
       "      <th>Ü</th>\n",
       "      <th>Üll</th>\n",
       "      <th>é</th>\n",
       "      <th>ü</th>\n",
       "      <th>üll</th>\n",
       "      <th>〨ud</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>160</td>\n",
       "      <td>2.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>128</td>\n",
       "      <td>4.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>49</td>\n",
       "      <td>4.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>62</td>\n",
       "      <td>3.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28</td>\n",
       "      <td>7.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 11041 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   body_len  punct%         0  008704050406  0089my  0121  01223585236  \\\n",
       "0       160     2.5  0.0  0.0           0.0     0.0   0.0          0.0   \n",
       "1       128     4.7  0.0  0.0           0.0     0.0   0.0          0.0   \n",
       "2        49     4.1  0.0  0.0           0.0     0.0   0.0          0.0   \n",
       "3        62     3.2  0.0  0.0           0.0     0.0   0.0          0.0   \n",
       "4        28     7.1  0.0  0.0           0.0     0.0   0.0          0.0   \n",
       "\n",
       "   01223585334  0125698789  ...  zero  zhong  zoom  zyada    Ü  Üll    é    ü  \\\n",
       "0          0.0         0.0  ...   0.0    0.0   0.0    0.0  0.0  0.0  0.0  0.0   \n",
       "1          0.0         0.0  ...   0.0    0.0   0.0    0.0  0.0  0.0  0.0  0.0   \n",
       "2          0.0         0.0  ...   0.0    0.0   0.0    0.0  0.0  0.0  0.0  0.0   \n",
       "3          0.0         0.0  ...   0.0    0.0   0.0    0.0  0.0  0.0  0.0  0.0   \n",
       "4          0.0         0.0  ...   0.0    0.0   0.0    0.0  0.0  0.0  0.0  0.0   \n",
       "\n",
       "   üll  〨ud  \n",
       "0  0.0  0.0  \n",
       "1  0.0  0.0  \n",
       "2  0.0  0.0  \n",
       "3  0.0  0.0  \n",
       "4  0.0  0.0  \n",
       "\n",
       "[5 rows x 11041 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_tfidf_feat = pd.DataFrame(X_tfidf.toarray())\n",
    "X_tfidf_feat.columns = tfidf_vect.get_feature_names()\n",
    "X_tfidf_feat=pd.concat([data['body_len'], data['punct%'], X_tfidf_feat],axis =1)\n",
    "X_tfidf_feat.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Section Summary: we have the data represented in two ways: X_counts_feat and X_tfidf_feat_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 5: Building Machine Learning Classifiers\n",
    "### Tool 1: A basic Random Forest model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_RF(n_est, depth):\n",
    "    rf = RandomForestClassifier(n_estimators=n_est, max_depth=depth, n_jobs=-1)\n",
    "    rf_model = rf.fit(X_train, y_train)\n",
    "    y_pred = rf_model.predict(X_test)\n",
    "    precision, recall, fscore, support = score(y_test, y_pred, pos_label='spam', average='binary')\n",
    "    print('Est: {} / Depth: {} ---- Precision: {} / Recall: {} / Accuracy: {}'.format(\n",
    "        n_est, depth, round(precision, 3), round(recall, 3),\n",
    "        round((y_pred==y_test).sum() / len(y_pred), 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using TF-IDF Method\n",
      "Est: 10 / Depth: 30 ---- Precision: 1.0 / Recall: 0.599 / Accuracy: 0.943\n",
      "Est: 10 / Depth: 60 ---- Precision: 1.0 / Recall: 0.847 / Accuracy: 0.978\n",
      "Est: 10 / Depth: 90 ---- Precision: 0.985 / Recall: 0.854 / Accuracy: 0.978\n",
      "Est: 10 / Depth: None ---- Precision: 1.0 / Recall: 0.803 / Accuracy: 0.972\n",
      "Est: 150 / Depth: 30 ---- Precision: 1.0 / Recall: 0.675 / Accuracy: 0.954\n",
      "Est: 150 / Depth: 60 ---- Precision: 1.0 / Recall: 0.815 / Accuracy: 0.974\n",
      "Est: 150 / Depth: 90 ---- Precision: 1.0 / Recall: 0.854 / Accuracy: 0.979\n",
      "Est: 150 / Depth: None ---- Precision: 1.0 / Recall: 0.841 / Accuracy: 0.978\n",
      "Est: 300 / Depth: 30 ---- Precision: 1.0 / Recall: 0.669 / Accuracy: 0.953\n",
      "Est: 300 / Depth: 60 ---- Precision: 1.0 / Recall: 0.809 / Accuracy: 0.973\n",
      "Est: 300 / Depth: 90 ---- Precision: 1.0 / Recall: 0.847 / Accuracy: 0.978\n",
      "Est: 300 / Depth: None ---- Precision: 1.0 / Recall: 0.866 / Accuracy: 0.981\n"
     ]
    }
   ],
   "source": [
    "print('using TF-IDF Method')\n",
    "X_features = X_tfidf_feat\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_features, data['label'], test_size=0.2)\n",
    "\n",
    "for n_est in [10, 150, 300]:\n",
    "    for depth in [30, 60, 90, None]:\n",
    "        train_RF(n_est, depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using CountVectorizer Method\n",
      "Est: 10 / Depth: 30 ---- Precision: 1.0 / Recall: 0.658 / Accuracy: 0.952\n",
      "Est: 10 / Depth: 60 ---- Precision: 1.0 / Recall: 0.748 / Accuracy: 0.965\n",
      "Est: 10 / Depth: 90 ---- Precision: 0.992 / Recall: 0.794 / Accuracy: 0.97\n",
      "Est: 10 / Depth: None ---- Precision: 1.0 / Recall: 0.723 / Accuracy: 0.961\n",
      "Est: 150 / Depth: 30 ---- Precision: 1.0 / Recall: 0.639 / Accuracy: 0.95\n",
      "Est: 150 / Depth: 60 ---- Precision: 1.0 / Recall: 0.761 / Accuracy: 0.967\n",
      "Est: 150 / Depth: 90 ---- Precision: 1.0 / Recall: 0.787 / Accuracy: 0.97\n",
      "Est: 150 / Depth: None ---- Precision: 1.0 / Recall: 0.787 / Accuracy: 0.97\n",
      "Est: 300 / Depth: 30 ---- Precision: 1.0 / Recall: 0.652 / Accuracy: 0.952\n",
      "Est: 300 / Depth: 60 ---- Precision: 1.0 / Recall: 0.761 / Accuracy: 0.967\n",
      "Est: 300 / Depth: 90 ---- Precision: 1.0 / Recall: 0.768 / Accuracy: 0.968\n",
      "Est: 300 / Depth: None ---- Precision: 1.0 / Recall: 0.787 / Accuracy: 0.97\n"
     ]
    }
   ],
   "source": [
    "print('using CountVectorizer Method')\n",
    "X_features = X_counts_feat\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_features, data['label'], test_size=0.2)\n",
    "\n",
    "for n_est in [10, 150, 300]:\n",
    "    for depth in [30, 60, 90, None]:\n",
    "        train_RF(n_est, depth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tool 2: A basic Gradient Boosting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_GB(est, max_depth, lr):\n",
    "    gb = GradientBoostingClassifier(n_estimators=est, max_depth=max_depth, learning_rate=lr)\n",
    "    gb_model = gb.fit(X_train, y_train)\n",
    "    y_pred = gb_model.predict(X_test)\n",
    "    precision, recall, fscore, train_support = score(y_test, y_pred, pos_label='spam', average='binary')\n",
    "    print('Est: {} / Depth: {} / LR: {} ---- Precision: {} / Recall: {} / Accuracy: {}'.format(\n",
    "        est, max_depth, lr, round(precision, 3), round(recall, 3), \n",
    "        round((y_pred==y_test).sum()/len(y_pred), 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using TF-IDF Method\n",
      "Est: 100 / Depth: 7 / LR: 0.1 ---- Precision: 0.973 / Recall: 0.838 / Accuracy: 0.971\n",
      "Est: 100 / Depth: 11 / LR: 0.1 ---- Precision: 0.973 / Recall: 0.832 / Accuracy: 0.97\n",
      "Est: 100 / Depth: 15 / LR: 0.1 ---- Precision: 0.974 / Recall: 0.85 / Accuracy: 0.973\n",
      "Est: 150 / Depth: 7 / LR: 0.1 ---- Precision: 0.973 / Recall: 0.838 / Accuracy: 0.971\n",
      "Est: 150 / Depth: 11 / LR: 0.1 ---- Precision: 0.973 / Recall: 0.838 / Accuracy: 0.971\n",
      "Est: 150 / Depth: 15 / LR: 0.1 ---- Precision: 0.973 / Recall: 0.838 / Accuracy: 0.971\n"
     ]
    }
   ],
   "source": [
    "print('using TF-IDF Method')\n",
    "X_features = X_tfidf_feat\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_features, data['label'], test_size=0.2)\n",
    "\n",
    "for n_est in [100, 150]:\n",
    "    for max_depth in [7, 11, 15]:\n",
    "        train_GB(n_est, max_depth, lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using CountVectorizer Method\n",
      "Est: 100 / Depth: 7 / LR: 0.1 ---- Precision: 0.972 / Recall: 0.839 / Accuracy: 0.979\n",
      "Est: 100 / Depth: 11 / LR: 0.1 ---- Precision: 0.963 / Recall: 0.847 / Accuracy: 0.979\n",
      "Est: 100 / Depth: 15 / LR: 0.1 ---- Precision: 0.964 / Recall: 0.855 / Accuracy: 0.98\n",
      "Est: 150 / Depth: 7 / LR: 0.1 ---- Precision: 0.972 / Recall: 0.831 / Accuracy: 0.978\n",
      "Est: 150 / Depth: 11 / LR: 0.1 ---- Precision: 0.981 / Recall: 0.855 / Accuracy: 0.982\n",
      "Est: 150 / Depth: 15 / LR: 0.1 ---- Precision: 0.981 / Recall: 0.847 / Accuracy: 0.981\n"
     ]
    }
   ],
   "source": [
    "print('using CountVectorizer Method')\n",
    "X_features = X_counts_feat\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_features, data['label'], test_size=0.2)\n",
    "\n",
    "for n_est in [100, 150]:\n",
    "    for max_depth in [7, 11, 15]:\n",
    "        train_GB(n_est, max_depth, lr=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion Section: Final Implementation\n",
    "_for the Random Forest Classifier: the best fit was having 150 n_estimators with a max_depth of None and enabling paralelism by having n_jobs equal to -1_<br>\n",
    "_for the Gradient Boosting Classifier: the best fit was also having 150 n_estimators and a max_depth of 11_\n",
    "\n",
    "In this section, the whole process is repeated from start to end while hiding the training part of the dataset from the very beginning and using only these classifiers with the specific findings from the previous sections measuring time to fit and evaluate a text as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data[['body_text', 'body_len', 'punct%']], data['label'], test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorize Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body_len</th>\n",
       "      <th>punct%</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>...</th>\n",
       "      <th>10164</th>\n",
       "      <th>10165</th>\n",
       "      <th>10166</th>\n",
       "      <th>10167</th>\n",
       "      <th>10168</th>\n",
       "      <th>10169</th>\n",
       "      <th>10170</th>\n",
       "      <th>10171</th>\n",
       "      <th>10172</th>\n",
       "      <th>10173</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>47</td>\n",
       "      <td>6.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>22</td>\n",
       "      <td>4.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.356204</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>30</td>\n",
       "      <td>6.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>27</td>\n",
       "      <td>3.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>24</td>\n",
       "      <td>4.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 10176 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   body_len  punct%    0    1    2    3    4    5    6    7  ...  10164  \\\n",
       "0        47     6.4  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...    0.0   \n",
       "1        22     4.5  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...    0.0   \n",
       "2        30     6.7  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...    0.0   \n",
       "3        27     3.7  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...    0.0   \n",
       "4        24     4.2  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...    0.0   \n",
       "\n",
       "   10165  10166  10167  10168  10169  10170  10171     10172  10173  \n",
       "0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  0.000000    0.0  \n",
       "1    0.0    0.0    0.0    0.0    0.0    0.0    0.0  0.356204    0.0  \n",
       "2    0.0    0.0    0.0    0.0    0.0    0.0    0.0  0.000000    0.0  \n",
       "3    0.0    0.0    0.0    0.0    0.0    0.0    0.0  0.000000    0.0  \n",
       "4    0.0    0.0    0.0    0.0    0.0    0.0    0.0  0.000000    0.0  \n",
       "\n",
       "[5 rows x 10176 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_vect = TfidfVectorizer(analyzer=clean_text)\n",
    "tfidf_vect_fit = tfidf_vect.fit(X_train['body_text'])\n",
    "\n",
    "tfidf_train = tfidf_vect_fit.transform(X_train['body_text'])\n",
    "tfidf_test = tfidf_vect_fit.transform(X_test['body_text'])\n",
    "\n",
    "X_train_vect = pd.concat([X_train[['body_len', 'punct%']].reset_index(drop=True), \n",
    "           pd.DataFrame(tfidf_train.toarray())], axis=1)\n",
    "X_test_vect = pd.concat([X_test[['body_len', 'punct%']].reset_index(drop=True), \n",
    "           pd.DataFrame(tfidf_test.toarray())], axis=1)\n",
    "\n",
    "X_train_vect.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final evaluation of models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit time: 6.166 / Predict time: 0.243 ---- Precision: 0.992 / Recall: 0.913 / Accuracy: 0.988\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "rf = RandomForestClassifier(n_estimators=150, max_depth=None, n_jobs=-1)\n",
    "\n",
    "start = time.time()\n",
    "rf_model = rf.fit(X_train_vect, y_train)\n",
    "end = time.time()\n",
    "fit_time = (end - start)\n",
    "\n",
    "start = time.time()\n",
    "y_pred = rf_model.predict(X_test_vect)\n",
    "end = time.time()\n",
    "pred_time = (end - start)\n",
    "\n",
    "precision, recall, fscore, train_support = score(y_test, y_pred, pos_label='spam', average='binary')\n",
    "print('Fit time: {} / Predict time: {} ---- Precision: {} / Recall: {} / Accuracy: {}'.format(\n",
    "    round(fit_time, 3), round(pred_time, 3), round(precision, 3), round(recall, 3), round((y_pred==y_test).sum()/len(y_pred), 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit time: 332.589 / Predict time: 0.257 ---- Precision: 0.984 / Recall: 0.877 / Accuracy: 0.983\n"
     ]
    }
   ],
   "source": [
    "gb = GradientBoostingClassifier(n_estimators=150, max_depth=11)\n",
    "\n",
    "start = time.time()\n",
    "gb_model = gb.fit(X_train_vect, y_train)\n",
    "end = time.time()\n",
    "fit_time = (end - start)\n",
    "\n",
    "start = time.time()\n",
    "y_pred = gb_model.predict(X_test_vect)\n",
    "end = time.time()\n",
    "pred_time = (end - start)\n",
    "\n",
    "precision, recall, fscore, train_support = score(y_test, y_pred, pos_label='spam', average='binary')\n",
    "print('Fit time: {} / Predict time: {} ---- Precision: {} / Recall: {} / Accuracy: {}'.format(\n",
    "    round(fit_time, 3), round(pred_time, 3), round(precision, 3), round(recall, 3), round((y_pred==y_test).sum()/len(y_pred), 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
